# BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding

*Issue*: the major limitation is that standard language models are unidirectional
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTAyMDI3NjUyXX0=
-->