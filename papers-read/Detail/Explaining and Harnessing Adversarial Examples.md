# [Explaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572.pdf)

Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy Google Inc., Mountain View, CA {goodfellow,shlens,szegedy}@google.com

Published as a conference paper at ICLR 2015

>Generic regularization strategies such as dropout, pretraining, and model averaging do not confer a significant reduction in a modelâ€™s vulnerability to adversarial examples, but changing to nonlinear model families such as RBF networks can do so.

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ4MTgyOTY1NF19
-->