# [Explaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572.pdf)

Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy Google Inc., Mountain View, CA {goodfellow,shlens,szegedy}@google.com

Published as a conference paper at ICLR 2015

>Generic regularization strategies such as dropout, pretraining, and model averaging do not confer a significant reduction in a modelâ€™s vulnerability to adversarial examples, but changing to nonlinear model families such as RBF networks can do so.

>More nonlinear models such as sigmoid networks are carefully tuned to spend most of their time in the non-saturating, more linear regime for the same reason
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTM0ODAzOTk1OSwtNDgxODI5NjU0XX0=
-->