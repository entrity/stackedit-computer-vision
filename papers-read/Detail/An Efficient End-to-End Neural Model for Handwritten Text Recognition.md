https://arxiv.org/pdf/1807.07965.pdf
# An Efficient End-to-End Neural Model for Handwritten Text Recognition

Prev state of the art: combines a Convolutional Neural Network ( CNN ) with a deep one-dimensional RNN-CTC model and holds the current state-of-the-art performance on standard HTR benchmarks.

Approach: combines a convolutional network as a feature extractor with two recurrent networks on top for sequence matching. We use the RNN based Encoder-Decoder network

> The context vector that forms a link between the encoder and the decoder often becomes an information bottleneck. Attention models are an extension to the standard encoder-
decoder framework in which the context vector is modified at each timestep based on the
decoder with the sequence of annotations similarity of the previous decoder hidden state h t−1
encoder encoder } generated by the encoder, for a given input sequence

>Additionally, we incorporate the attention input feeding approach used in Luong [19] attention mechanism in which the context vector from previous timestep is concatenated with the input of the current timestep. It helps in building a local context, further augmenting the predictive capacity of the network

> **[Cross Entropy Loss]** often suffers from **class imbalance problem**. In such a situation, the CE loss is mostly composed of the easily classified examples which dominate the gradient. **Focal Loss** [18] addresses this problem by assigning suitable weights to the contribution of each instance in the final loss. It is defined as $FL(p) = -(1 - p)^\gamma log(p)$, where p is the true-class probability and γ is a tunable focusing parameter ... Harder-to-classify examples get more weight.thereby making larger updates for the hard examples.

> **Greedy Decoding ( GD )** which emits, at each timestep, the class with the highest probability from the softmax distribution, as the output at that instance

> **Beam Search** which aims to find the best sequence by maximizing the joint distribution
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTU3NTkyMjc1M119
-->